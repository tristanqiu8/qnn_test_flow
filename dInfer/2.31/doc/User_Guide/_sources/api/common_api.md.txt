# 基本API使用

请参考以下测试代码，了解如何使用dInfer API。

## MNN

**对于`DINFER_MNN`runtime，默认开启了fallback逻辑：即当模型在指定device(如`DINFER_GPU`)上不支持时，会自动使用`DINFER_CPU`进行推理，而不会返回错误。若希望明确返回错误，请参考[MNN高阶API使用](mnn_api.md)**

### C++ API Demo

```c++
// dInfer C++ API
#include "dInfer/dInfer_api.h"

// test helper
#include "file_path.h"
#include "test_utils.h"
#include "test/src/post_process/parsing.h"

// third party
#include "gtest/gtest.h"
#include "opencv2/imgcodecs.hpp"
#include "opencv2/dnn/dnn.hpp"

// std
#include <string>
using namespace std;

/**
 * @brief 基于MNN Runtime演示dInfer的基础使用
 * 
 */
TEST(mnn_runtime, basic_demo) {
    string test_result_folder = GetCurrentTestDir();

    // 1. 配置模型信息
    dInferModelInfo info;
    info.device = dInferDevice::DINFER_CPU;
    info.runtime = dInferEngine::DINFER_MNN;
    info.model_path = std::string(MODELZOO_PATH) + "water/water.mnn";
    info.model_encrypt = false;

    // 2. 创建dInfer实例
    dInferInterface *infer = dInferInterfaceCreate(&info);
    ASSERT_NE(infer, nullptr);

    // 3. 获取输入tensor
    // 可直接访问tensor列表，但【不推荐】通过索引顺序获取tensor，模型转换可能导致tensor顺序发生变化
    EXPECT_EQ(infer->input_tensor_.size(), 1);

    // 【推荐】通过名称对tensor进行索引
    dInferTensor *input_tensor = infer->GetTensor("input.1");
    EXPECT_NE(input_tensor, nullptr);
    
    // 不存在的tensor会返回nullptr
    dInferTensor *bad_tensor = infer->GetTensor("bad_tensor");
    EXPECT_EQ(bad_tensor, nullptr);

    // 注意：检查tensor的shape、data_type等信息
    EXPECT_EQ(input_tensor->rank, 4);
    int n = input_tensor->dim[0];
    int c = input_tensor->dim[1];
    int h = input_tensor->dim[2];
    int w = input_tensor->dim[3];
    EXPECT_EQ(n, 1);
    EXPECT_EQ(c, 3);
    EXPECT_EQ(h, 1824);
    EXPECT_EQ(w, 2432);
    EXPECT_EQ(input_tensor->data_type, DINFER_F32);
    // NOTE: layout字段已被弃用，不建议前后处理依据该信息解释Tensor
    //       前后处理应该依据模型转换时确定的layout解释Tensor
    // EXPECT_EQ(input_tensor->layout, DINFER_NHWC);
    
    // 注意：检查内存地址和大小
    EXPECT_EQ(input_tensor->data.type, dInferMemType::DINFER_MEM_HOST_BUF);
    void *input_buffer = (void *)input_tensor->data.host_addr;
    size_t input_buffer_size = input_tensor->data.size;
    EXPECT_NE(input_buffer, nullptr);
    EXPECT_EQ(input_buffer_size, n * c * h * w * sizeof(float));

    // 4. 数据加载和前处理，具体实现取决于具体业务
    cv::Mat img = cv::imread(std::string(DATA_PATH) + "water/water.jpg");
    EXPECT_NE(img.empty(), true);
    cv::Mat blob = cv::dnn::blobFromImage(img, 1.0/128.0, cv::Size(w, h), cv::Scalar(128, 128, 128), true, false, CV_32F);

    // 5. 填充输入数据
    // 注意：确保buffer size一致
    size_t blob_size = blob.total() * blob.elemSize();
    EXPECT_EQ(blob_size, input_buffer_size);
    memcpy(input_buffer, blob.data, input_buffer_size);

    // 4. 推理
    EXPECT_EQ(infer->Process(), DINFER_OK);

    // 5. 获取输出数据
    dInferTensor *output_tensor = infer->GetTensor("430");
    EXPECT_NE(output_tensor, nullptr);

    // 6. 后处理，具体实现取决于业务
    std::string out_file = test_result_folder + "/mnn_basic_demo_out.jpg";
    EXPECT_EQ(ParsingRender(img, output_tensor, out_file), true);

    // 7. 释放资源
    EXPECT_EQ(dInferInterfaceDestroy(infer), DINFER_OK);
}

```

### C API Demo

```c
// dInfer C API
#include "dInfer/dInfer_api_c.h"

// test helper
#include "file_path.h"
#include "test_utils.h"
#include "test/src/post_process/parsing.h"

// third party
#include "gtest/gtest.h"
#include "opencv2/imgcodecs.hpp"
#include "opencv2/dnn/dnn.hpp"

// std
#include <string>
using namespace std;

/**
 * @brief 基于MNN Runtime演示dInfer C API的基础使用
 * 
 */
TEST(mnn_runtime, basic_c_demo) {
    string test_result_folder = GetCurrentTestDir();
    string model_path = std::string(MODELZOO_PATH) + "water.mnn";
    string img_path = std::string(DATA_PATH) + "water.jpg";

    // 1. 配置模型信息
    dInferModelInfo_t model_info;
    model_info.runtime = "MNN"; // TODO: 目前无法选择device, 固定为CPU。
    model_info.model_path = (char*)model_path.c_str();
    model_info.model_encrypt = false;

    // 2.1 创建TensorInfo结构体
    dInferTensorInfo_t *tensor_info = (dInferTensorInfo_t*)malloc(sizeof(dInferTensorInfo_t));
    ASSERT_NE(tensor_info, nullptr);
    memset(tensor_info, 0, sizeof(dInferTensorInfo_t));

    // 2.2 创建dInfer实例
    void *infer = dInferInterfaceCreate(&model_info, tensor_info);
    ASSERT_NE(infer, nullptr);

    // 3. 获取输入tensor
    EXPECT_EQ(tensor_info->input_num, 1);
    // 【推荐】通过名称对tensor进行索引
    dInferTensor_t *input_tensor = dInferGetTensor(tensor_info, "input.1");
    ASSERT_NE(input_tensor, nullptr);

    // 不存在的tensor会返回nullptr
    dInferTensor_t *bad_tensor = dInferGetTensor(tensor_info, "bad_tensor");
    EXPECT_EQ(bad_tensor, nullptr);

    // 注意：检查tensor的shape、data_type等信息
    EXPECT_EQ(input_tensor->rank, 4);
    int n = input_tensor->dim[0];
    int c = input_tensor->dim[1];
    int h = input_tensor->dim[2];
    int w = input_tensor->dim[3];
    EXPECT_EQ(n, 1);
    EXPECT_EQ(c, 3);
    EXPECT_EQ(h, 1824);
    EXPECT_EQ(w, 2432);
    EXPECT_EQ(input_tensor->data_type, DINFER_F32);
    // NOTE: layout字段已被弃用，不建议前后处理依据该信息解释Tensor
    //       前后处理应该依据模型转换时确定的layout解释Tensor
    // EXPECT_EQ(input_tensor->layout, DINFER_NHWC);
    
    // 注意：检查内存地址和大小
    EXPECT_EQ(input_tensor->data.type, DINFER_MEM_HOST_BUF);
    void *input_buffer = (void *)input_tensor->data.host_addr;
    size_t input_buffer_size = input_tensor->data.size;
    EXPECT_NE(input_buffer, nullptr);
    EXPECT_EQ(input_buffer_size, n * c * h * w * sizeof(float));

    // 4. 数据加载和前处理
    cv::Mat img = cv::imread(img_path);
    ASSERT_NE(img.empty(), true) << "Failed to read image from " << img_path;
    cv::Mat blob = cv::dnn::blobFromImage(img, 1.0/128.0, cv::Size(w, h), cv::Scalar(128, 128, 128), true, false, CV_32F);

    // 5. 填充输入数据
    // 注意：确保buffer size一致
    size_t blob_size = blob.total() * blob.elemSize();
    EXPECT_EQ(blob_size, input_buffer_size);
    memcpy(input_buffer, blob.data, input_buffer_size);

    // 6. 推理
    EXPECT_EQ(dInferInterfaceProcessSync(infer, tensor_info), DINFER_OK);

    // 7. 获取输出数据
    EXPECT_EQ(tensor_info->output_num, 1);
    dInferTensor_t *output_tensor = dInferGetTensor(tensor_info, "430");
    ASSERT_NE(output_tensor, nullptr);

    // 8. 后处理
    std::string out_file = test_result_folder + "/mnn_demo_out.jpg";
    EXPECT_EQ(ParsingRender(img, output_tensor, out_file), true);

    // 9. 释放资源
    EXPECT_EQ(dInferInterfaceDestroy(infer, tensor_info), DINFER_OK);
    free(tensor_info);
}
```

## TensorRT

### C++ API Demo

```c++
// dInfer API
#include "dInfer/dInfer_api.h"

// test helper
#include "file_path.h"
#include "test_utils.h"
#include "test/src/post_process/parsing.h"

// third party
#include "gtest/gtest.h"
#include "opencv2/imgcodecs.hpp"
#include "opencv2/dnn/dnn.hpp"

// std
#include <string>
using namespace std;

/**
 * @brief 基于TensorRT Runtime演示dInfer的基础使用
 * 
 */
TEST(trt_runtime, basic_demo) {
    string test_result_folder = GetCurrentTestDir();

    string model_path = std::string(MODELZOO_PATH) + "water.onnx";
    string model_cache_path = std::string(MODELZOO_PATH) + "water.trtModel";
    string img_path = std::string(DATA_PATH) + "water.jpg";

    // 1. 配置模型信息
    dInferModelInfo info;
    info.device = dInferDevice::DINFER_GPU;
    info.runtime = dInferEngine::DINFER_TRT;
    info.model_path = model_path;
    info.model_encrypt = false;
    // 设置模型缓存以加速初始化
    info.optional_attrs["trt_cache_path"] = (void*)model_cache_path.c_str();

    // 2. 创建dInfer实例
    std::shared_ptr<dInferInterface> infer(dInferInterfaceCreate(&info), [](dInferInterface* p) {
        // 7. 释放资源
        EXPECT_EQ(dInferInterfaceDestroy(p), DINFER_OK);
    });
    ASSERT_NE(infer, nullptr);

    // 3. 获取输入tensor
    EXPECT_EQ(infer->input_tensor_.size(), 1);
    dInferTensor *input_tensor = infer->GetTensor("input.1");
    ASSERT_NE(input_tensor, nullptr);
    // 不存在的tensor会返回nullptr
    dInferTensor *bad_tensor = infer->GetTensor("bad_tensor");
    EXPECT_EQ(bad_tensor, nullptr);
    // 检查tensor的shape、data_type等信息
    EXPECT_EQ(input_tensor->rank, 4);
    int input_n = input_tensor->dim[0];
    int input_c = input_tensor->dim[1];
    int input_h = input_tensor->dim[2];
    int input_w = input_tensor->dim[3];
    EXPECT_EQ(input_n, 1);
    EXPECT_EQ(input_h, 1824);
    EXPECT_EQ(input_w, 2432);
    EXPECT_EQ(input_c, 3);
    EXPECT_EQ(input_tensor->data_type, DINFER_F32);
    // 检查内存地址和大小
    EXPECT_EQ(input_tensor->data.type, dInferMemType::DINFER_MEM_HOST_BUF);
    void *input_buffer = (void *)input_tensor->data.host_addr;
    size_t input_buffer_size = input_tensor->data.size;
    EXPECT_NE(input_buffer, nullptr);
    EXPECT_EQ(input_buffer_size, input_n * input_c * input_h * input_w * sizeof(float));

    // 4. 数据加载和前处理，具体实现取决于具体业务
    cv::Mat img = cv::imread(img_path);
    EXPECT_NE(img.empty(), true);
    cv::Mat blob = cv::dnn::blobFromImage(img, 1.0/128.0, cv::Size(input_w, input_h), cv::Scalar(128, 128, 128), true, false, CV_32F);

    // 5. 填充输入数据
    // 注意：确保buffer size一致
    size_t blob_size = blob.total() * blob.elemSize();
    ASSERT_EQ(blob_size, input_buffer_size);
    memcpy(input_buffer, blob.data, input_buffer_size);

    // 4. 推理
    EXPECT_EQ(infer->Process(), DINFER_OK);

    // 5. 获取输出数据
    dInferTensor *output_tensor = infer->GetTensor("430");
    EXPECT_NE(output_tensor, nullptr);

    // 6. 后处理，具体实现取决于业务
    string out_file = test_result_folder + "/basic_demo_out.jpg";
    EXPECT_EQ(ParsingRender(img, output_tensor, out_file), true);

    // 7. 释放资源，这里使用了智能指针，超出作用域后会自动释放
}
```

## QNN

**`DINFER_QNN`runtime支持`模型库`和`模型缓存`两种加载方式，请参考[QNN高阶API使用](qnn_api_v2.md)**

### C++ API Demo

```c++
// dInfer C++ API
#include "dInfer/dInfer_api.h"

// test helper
#include "dInfer/utils.h"
#include "utils/common.h"
#include "file_path.h"
#include "test_utils.h"
#include "test/src/post_process/parsing.h"

// third party
#include "gtest/gtest.h"
#include "opencv2/imgcodecs.hpp"

// std
#include <string>
using namespace std;

/**
 * @brief 基于QNN Runtime演示dInfer的基础使用，加载序列化后的模型文件
 * 
 */
TEST(qnn_runtime, serialized_model_demo) {
    string test_result_folder = GetCurrentTestDir();

    // 1. 配置模型信息
    dInferModelInfo info;
    info.device = dInferDevice::DINFER_HTP;
    info.runtime = dInferEngine::DINFER_QNN;
    info.model_path = string(MODELZOO_PATH) + "water_229_htp_vtcm8mb.serialized.bin";
    info.model_encrypt = false;
    // 加载【序列化模型】时，需要指定该选项，由于是指针传递，使用时请注意变量生存期。
    bool load_from_cache = true;
    info.optional_attrs["qnn_load_from_cache"] = &load_from_cache;

    // 2. 创建dInfer实例
    std::shared_ptr<dInferInterface> infer(dInferInterfaceCreate(&info), [](dInferInterface* p) {
        // 7. 释放资源
        EXPECT_EQ(dInferInterfaceDestroy(p), DINFER_OK);
    });
    ASSERT_NE(infer, nullptr);

    // 3. 获取输入tensor
    // 可直接访问tensor列表，但【不推荐】通过索引顺序获取tensor，模型转换可能导致tensor顺序发生变化
    EXPECT_EQ(infer->input_tensor_.size(), 1);

    // 【推荐】通过名称对tensor进行索引
    dInferTensor *input_tensor = infer->GetTensor("input_1");
    ASSERT_NE(input_tensor, nullptr);
    
    // 不存在的tensor会返回nullptr
    dInferTensor *bad_tensor = infer->GetTensor("bad_tensor");
    EXPECT_EQ(bad_tensor, nullptr);

    // 注意：检查tensor的shape、data_type等信息
    // 注意：QNN转换的模型，shape可能与源模型不一致，请以模型转换时生成的json文件来解释layout
    EXPECT_EQ(input_tensor->rank, 4);
    int input_n = input_tensor->dim[0];
    int input_h = input_tensor->dim[1];
    int input_w = input_tensor->dim[2];
    int input_c = input_tensor->dim[3];
    EXPECT_EQ(input_n, 1);
    EXPECT_EQ(input_h, 1824);
    EXPECT_EQ(input_w, 2432);
    EXPECT_EQ(input_c, 3);
    EXPECT_EQ(input_tensor->data_type, DINFER_U8);
    EXPECT_NE(input_tensor->quant, nullptr) << "量化模型没有获取到量化参数!";
    // NOTE: layout字段已被弃用，不建议前后处理依据该信息解释Tensor
    //       前后处理应该依据模型转换时确定的layout解释Tensor
    // EXPECT_EQ(input_tensor->layout, DINFER_NHWC);
    
    // 注意：检查内存地址和大小
    EXPECT_EQ(input_tensor->data.type, dInferMemType::DINFER_MEM_HOST_BUF);
    void *input_buffer = (void *)input_tensor->data.host_addr;
    size_t input_buffer_size = input_tensor->data.size;
    EXPECT_NE(input_buffer, nullptr);
    EXPECT_EQ(input_buffer_size, input_n * input_c * input_h * input_w * sizeof(uint8_t));

    // 4. 数据加载和前处理，具体实现取决于具体业务
    string img_path = std::string(DATA_PATH) + "water.jpg";
    cv::Mat img = cv::imread(img_path);
    ASSERT_NE(img.empty(), true) << "图片加载失败: " << img_path;
    EXPECT_EQ(img.channels(), input_c);
    EXPECT_EQ(img.rows, input_h);
    EXPECT_EQ(img.cols, input_w);
    // 注意：量化模型输入需要做量化，或等效的转换
    float input_scale;
    int input_offset;
    GetQuantParam(*input_tensor, &input_scale, &input_offset);
    LOGI("input: scale = %f, zeropoint = %d\n", input_scale, input_offset);
    cv::Mat norm_img;
    img.convertTo(norm_img, CV_32FC(input_c));
    norm_img = (norm_img - 128.0) / 128.0;
    cv::Mat quant_img = norm_img / input_scale - input_offset;
    quant_img.convertTo(quant_img, CV_8UC(input_c));

    // 5. 填充输入数据
    // 注意：确保buffer size一致
    size_t quant_img_size = quant_img.total() * quant_img.elemSize();
    ASSERT_EQ(quant_img_size, input_buffer_size);
    memcpy(input_buffer, quant_img.data, input_buffer_size);

    // 4. 推理
    EXPECT_EQ(infer->Process(), DINFER_OK);

    // 5. 获取输出数据
    dInferTensor *output_tensor = infer->GetTensor("_430");
    EXPECT_NE(output_tensor, nullptr);
    int output_n = output_tensor->dim[0];
    int output_h = output_tensor->dim[1];
    int output_w = output_tensor->dim[2];
    int output_c = output_tensor->dim[3];
    EXPECT_EQ(output_n, 1);
    EXPECT_EQ(output_h, 1824);
    EXPECT_EQ(output_w, 2432);
    EXPECT_EQ(output_c, 2);
    // 注意：量化模型输出需要反量化，或等效的转换
    float output_scale;
    int output_offset;
    GetQuantParam(*output_tensor, &output_scale, &output_offset);
    LOGI("output: scale = %f, zeropoint = %d\n", output_scale, output_offset);
    uint8_t *output_buffer = (uint8_t *)output_tensor->data.host_addr;
    size_t output_buffer_size = output_tensor->data.size;

    // buffer转cv::Mat
    cv::Mat quant_out(output_h, output_w, CV_8UC2, output_buffer);
    // 反量化
    cv::Mat float_out;
    quant_out.convertTo(float_out, CV_32FC2, output_scale, output_scale*output_offset);
    // 转成dInferTensor，注意这里的shape是nhwc
    dInferTensor quant_out_tensor = *output_tensor;
    quant_out_tensor.data.host_addr = (uint64_t)float_out.data;
    quant_out_tensor.data.size = float_out.total() * float_out.elemSize() * sizeof(float);
    quant_out_tensor.data_type = DINFER_F32;

    // 6. 后处理，具体实现取决于业务
    std::string out_file = test_result_folder + "/water_out.jpg";
    EXPECT_EQ(ParsingRender(img, &quant_out_tensor, out_file), true);

    // 7. 释放资源，这里使用了智能指针，超出作用域后会自动释放
}
```

### C API Demo

```c
// dInfer C API
#include "dInfer/dInfer_api_c.h"

// test helper
#include "dInfer/osal/log.h"
#include "dInfer/osal/time.h"
#include "dInfer/utils.h"
#include "utils/common.h"
#include "file_path.h"
#include "test_utils.h"
#include "test/src/post_process/parsing.h"
#include "core/utils/tensor_utils.h"

// third party
#include "gtest/gtest.h"
#include "opencv2/imgcodecs.hpp"

// std
#include <string>
using namespace std;

TEST(qnn_runtime, serialized_model_c_demo) {
    string test_result_folder = GetCurrentTestDir();
    string model_bin_file = string(MODELZOO_PATH) + "water_229_htp_vtcm8mb.serialized.bin";
    string img_path = std::string(DATA_PATH) + "water.jpg";

    // 1. 配置模型信息
    dInferModelInfo_t model_info;
    memset(&model_info, 0, sizeof(dInferModelInfo_t));
    model_info.runtime = "QNN"; // TODO: 目前无法选择device, 固定为HTP。
    model_info.model_path = (char*)model_bin_file.c_str();
    model_info.model_encrypt = false;
    model_info.optional_attrs_ql = true; // 加载【序列化模型】时，需要设置该选项。
    
    // 2.1 创建TensorInfo结构体
    dInferTensorInfo_t *tensor_info = (dInferTensorInfo_t*)malloc(sizeof(dInferTensorInfo_t));
    ASSERT_NE(tensor_info, nullptr);
    memset(tensor_info, 0, sizeof(dInferTensorInfo_t));

    // 2.2 创建dInfer实例
    void *infer = dInferInterfaceCreate(&model_info, tensor_info);
    ASSERT_NE(infer, nullptr);

    // 3. 获取输入tensor
    EXPECT_EQ(tensor_info->input_num, 1);
    // 【推荐】通过名称对tensor进行索引
    dInferTensor_t *input_tensor = dInferGetTensor(tensor_info, "input_1");
    ASSERT_NE(input_tensor, nullptr);

    // 不存在的tensor会返回nullptr
    dInferTensor_t *bad_tensor = dInferGetTensor(tensor_info, "bad_tensor");
    EXPECT_EQ(bad_tensor, nullptr);

    // 注意：检查tensor的shape、data_type等信息
    // 注意：QNN转换的模型，shape可能与源模型不一致，请以模型转换时生成的json文件来解释layout
    EXPECT_EQ(input_tensor->rank, 4);
    int input_n = input_tensor->dim[0];
    int input_h = input_tensor->dim[1];
    int input_w = input_tensor->dim[2];
    int input_c = input_tensor->dim[3];
    EXPECT_EQ(input_n, 1);
    EXPECT_EQ(input_h, 1824);
    EXPECT_EQ(input_w, 2432);
    EXPECT_EQ(input_c, 3);
    EXPECT_EQ(input_tensor->data_type, DINFER_U8);
    EXPECT_NE(input_tensor->quant, nullptr) << "量化模型没有获取到量化参数!";
    // NOTE: layout字段已被弃用，不建议前后处理依据该信息解释Tensor
    //       前后处理应该依据模型转换时确定的layout解释Tensor
    // EXPECT_EQ(input_tensor->layout, DINFER_NHWC);

    // 注意：检查内存地址和大小
    EXPECT_EQ(input_tensor->data.type, dInferMemType::DINFER_MEM_HOST_BUF);
    void *input_buffer = (void *)input_tensor->data.host_addr;
    size_t input_buffer_size = input_tensor->data.size;
    EXPECT_NE(input_buffer, nullptr);
    EXPECT_EQ(input_buffer_size, input_n * input_c * input_h * input_w * sizeof(uint8_t));

    // 4. 数据加载和前处理，具体实现取决于具体业务
    cv::Mat img = cv::imread(img_path);
    ASSERT_NE(img.empty(), true) << "图片加载失败: " << img_path;
    EXPECT_EQ(img.channels(), input_c);
    EXPECT_EQ(img.rows, input_h);
    EXPECT_EQ(img.cols, input_w);
    // 注意：量化模型输入需要做量化，或等效的转换
    float input_scale;
    int input_offset;
    GetQuantParam(*input_tensor, &input_scale, &input_offset);
    LOGI("input: scale = %f, zeropoint = %d\n", input_scale, input_offset);
    cv::Mat norm_img;
    img.convertTo(norm_img, CV_32FC(input_c));
    norm_img = (norm_img - 128.0) / 128.0;
    cv::Mat quant_img = norm_img / input_scale - input_offset;
    quant_img.convertTo(quant_img, CV_8UC(input_c));

    // 5. 填充输入数据
    // 注意：确保buffer size一致
    size_t quant_img_size = quant_img.total() * quant_img.elemSize();
    ASSERT_EQ(quant_img_size, input_buffer_size);
    memcpy(input_buffer, quant_img.data, input_buffer_size);

    // 6. 推理
    EXPECT_EQ(dInferInterfaceProcessSync(infer, tensor_info), DINFER_OK);

    // 7. 获取输出tensor
    EXPECT_EQ(tensor_info->output_num, 1);
    dInferTensor_t *output_tensor = dInferGetTensor(tensor_info, "_430");
    ASSERT_NE(output_tensor, nullptr);
    int output_n = output_tensor->dim[0];
    int output_h = output_tensor->dim[1];
    int output_w = output_tensor->dim[2];
    int output_c = output_tensor->dim[3];
    EXPECT_EQ(output_n, 1);
    EXPECT_EQ(output_h, 1824);
    EXPECT_EQ(output_w, 2432);
    EXPECT_EQ(output_c, 2);
    EXPECT_EQ(output_tensor->data.type, dInferMemType::DINFER_MEM_HOST_BUF);
    // 注意：量化模型输出需要反量化，或等效的转换
    float output_scale;
    int output_offset;
    GetQuantParam(*output_tensor, &output_scale, &output_offset);
    LOGI("output: scale = %f, zeropoint = %d\n", output_scale, output_offset);
    uint8_t *output_buffer = (uint8_t *)output_tensor->data.host_addr;
    size_t output_buffer_size = output_tensor->data.size;

    // buffer转cv::Mat
    cv::Mat quant_out(output_h, output_w, CV_8UC2, output_buffer);
    // 反量化
    cv::Mat float_out;
    quant_out.convertTo(float_out, CV_32FC2, output_scale, output_scale*output_offset);
    // 转成dInferTensor，注意这里的shape是nhwc
    dInferTensor quant_out_tensor = *output_tensor;
    quant_out_tensor.data.host_addr = (uint64_t)float_out.data;
    quant_out_tensor.data.size = float_out.total() * float_out.elemSize() * sizeof(float);
    quant_out_tensor.data_type = DINFER_F32;

    // 8. 后处理，具体实现取决于业务
    std::string out_file = test_result_folder + "/water_out.jpg";
    EXPECT_EQ(ParsingRender(img, &quant_out_tensor, out_file), true);

    // 9. 释放资源
    EXPECT_EQ(dInferInterfaceDestroy(infer, tensor_info), DINFER_OK);
    free(tensor_info);
}
```

# 获取Tensor

有两种方式从dInfer实例中获取Tensor数据结构：

1. **通过名称获取，建议任何时候都使用该方式**。
2. 自然顺序索引，不推荐。因为模型转换过程中，转换后的目标模型（`QNN`、`MNN`等）的Tensor顺序可能与源模型（`ONNX`、`TorchScript`等）中的Tensor顺序不一致！

`C++ API`用户请使用`dInferInterface::GetTensor()`接口获取Tensor，示例：

```c++
#include "dInfer/dInfer_api.h"

using namespace dInfer;

TEST(api, get_tensor_cpp) {
    dInferModelInfo info;
    // 配置 model info...

    dInferInterface *infer = dInferInterfaceCreate(&info);

    // 获取输入tensor
    dInferTensor *input_tensor = infer->GetTensor("input_name");

    // 获取输出tensor
    dInferTensor *output_tensor = infer->GetTensor("output_name");

    // ...
}
```

`C API`用户请使用`dInferGetTensor()`接口获取Tensor。

```c
#include "dInfer/dInfer_api_c.h"

TEST(api, get_tensor_c) {
    dInferModelInfo_t model_info;
    // 配置 model info...

    dInferTensorInfo_t *tensor_info = new dInferTensorInfo_t;
    void *infer = dInferInterfaceCreate(&model_info, tensor_info);

    // 获取输入tensor
    dInferTensor_t *input_tensor = dInferGetTensor(tensor_info, "input_name");

    // 获取输出tensor
    dInferTensor_t *output_tensor = dInferGetTensor(tensor_info, "output_name");

    // ...

    delete tensor_info;
}

```

# 性能统计

使用profiler可以profiling模型初始化时间，推理时间以及内存占用. 通过optional_attrs中profiling关键字来启动。

形式： `{"profiling": 期望帧率<< 8 + warmup值}`. warmup是低8bit存储, 即最大值为255; 期望推理帧率可缺省，只能为整数, 默认为0, 表示不统计推理耗时稳定性。

   ```c++
   std::map<std::string, void *> optional_attrs = {
      std::make_pair<std::string, void *>("profiling", reinterpret_cast<void *>(/*warmup*/2 + /*期望帧率*/(30<<8)))
   };
   ```

在启动profiler后，还可以额外使能timelist功能，保存模型推理耗时情况

   ```c++
   info.optional_attrs["timelist"] = nullptr;
   ```

通过 `dInferInterface::GetProfile()` 接口就可以获取到profiler的性能信息
