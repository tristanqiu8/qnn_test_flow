# MNN高阶API使用

## 调度配置选项

在选择`DINFER_MNN`为runtime时，可以使用`schedule_config`选项设置MNN的调度配置，来精确控制推理类型、后端配置等。

```c++
#include "dInfer/dInfer_api_mnn.h"

// 设置调度配置
MNN::ScheduleConfig schedule_config;
// ...

// 创建dInfer时传入调度配置选项
dInferModelInfo info;
// ...
info.optional_attrs["schedule_config"] = reinterpret_cast<void *>(&schedule_config);
dInferInterface *infer = dInferInterfaceCreate(&info);   // NOTE: 注意生存期

// ...
```

**注意`schedule_config`的生存期，确保在创建完成前不被释放**

以下具体定义和用法，摘自：[MNN Docs - Session API使用](https://mnn-docs.readthedocs.io/en/latest/inference/session.html)

### 调度配置
调度配置定义如下：
```cpp
/** session schedule config */
struct ScheduleConfig {
    /** which tensor should be kept */
    std::vector<std::string> saveTensors;
    /** forward type */
    MNNForwardType type = MNN_FORWARD_CPU;
    /** CPU:number of threads in parallel , Or GPU: mode setting*/
    union {
        int numThread = 4;
        int mode;
    };

    /** subpath to run */
    struct Path {
        std::vector<std::string> inputs;
        std::vector<std::string> outputs;

        enum Mode {
            /**
             * Op Mode
             * - inputs means the source op, can NOT be empty.
             * - outputs means the sink op, can be empty.
             * The path will start from source op, then flow when encounter the sink op.
             * The sink op will not be compute in this path.
             */
            Op = 0,

            /**
             * Tensor Mode
             * - inputs means the inputs tensors, can NOT be empty.
             * - outputs means the outputs tensors, can NOT be empty.
             * It will find the pipeline that compute outputs from inputs.
             */
            Tensor = 1
        };

        /** running mode */
        Mode mode = Op;
    };
    Path path;

    /** backup backend used to create execution when desinated backend do NOT support any op */
    MNNForwardType backupType = MNN_FORWARD_CPU;

    /** extra backend config */
    BackendConfig* backendConfig = nullptr;
};
```

推理时，主选后端由`type`指定，默认为CPU。若模型中存在主选后端不支持的算子，这些算子会使用由`backupType`指定的备选后端运行。

推理路径包括由`path`的`inputs`到`outputs`途径的所有算子，在不指定时，会根据模型结构自动识别。为了节约内存，MNN会复用`outputs`之外的tensor内存。如果需要保留中间tensor的结果，可以使用`saveTensors`保留tensor结果，避免内存复用。

CPU推理时，并发数与线程数可以由`numThread`修改。`numThread`决定并发数的多少，但具体线程数和并发效率，不完全取决于`numThread`：

- iOS，线程数由系统GCD决定；
- 启用`MNN_USE_THREAD_POOL`时，线程数取决于第一次配置的大于1的`numThread`；
- OpenMP，线程数全局设置，实际线程数取决于最后一次配置的`numThread`；

GPU推理时，可以通过mode来设置GPU运行的一些参量选择(暂时只支持OpenCL)。GPU mode参数如下：
```c
typedef enum {
    // choose one tuning mode Only
    MNN_GPU_TUNING_NONE    = 1 << 0,/* Forbidden tuning, performance not good */
    MNN_GPU_TUNING_HEAVY  = 1 << 1,/* heavily tuning, usually not suggested */
    MNN_GPU_TUNING_WIDE   = 1 << 2,/* widely tuning, performance good. Default */
    MNN_GPU_TUNING_NORMAL = 1 << 3,/* normal tuning, performance may be ok */
    MNN_GPU_TUNING_FAST   = 1 << 4,/* fast tuning, performance may not good */
    
    // choose one opencl memory mode Only
    /* User can try OpenCL_MEMORY_BUFFER and OpenCL_MEMORY_IMAGE both, then choose the better one according to performance*/
    MNN_GPU_MEMORY_BUFFER = 1 << 6,/* User assign mode */
    MNN_GPU_MEMORY_IMAGE  = 1 << 7,/* User assign mode */
} MNNGpuMode;
```
目前支持tuning力度以及GPU memory用户可自由设置。例如：
```c
MNN::ScheduleConfig config;
config.mode = MNN_GPU_TUNING_NORMAL | MNN_GPU_MEMORY_IMAGE;
```
tuning力度选取越高，第一次初始化耗时越多，推理性能越佳。如果介意初始化时间过长，可以选取MNN_GPU_TUNING_FAST或者MNN_GPU_TUNING_NONE，也可以同时通过下面的cache机制，第二次之后就不会慢。GPU_Memory用户可以指定使用MNN_GPU_MEMORY_BUFFER或者MNN_GPU_MEMORY_IMAGE，用户可以选择性能更佳的那一种。如果不设定，框架会采取默认判断帮你选取(不保证一定性能最优)。

**上述CPU的numThread和GPU的mode，采用union联合体方式，共用同一片内存。用户在设置的时候numThread和mode只需要设置一种即可，不要重复设置。**

**对于GPU初始化较慢的问题，提供了Cache机制**。后续可以直接加载cache提升初始化速度。

- 具体可以参考tools/cpp/MNNV2Basic.cpp里面setCacheFile设置cache方法进行使用。
- 当模型推理输入尺寸有有限的多种时，每次resizeSession后调用updateCacheFile更新cache文件。
- 当模型推理输入尺寸无限随机变化时，建议config.mode设为1，关闭MNN_GPU_TUNING。


此外，可以通过`backendConfig`设定后端的额外参数。具体见下。

### 后端配置
后端配置定义如下：
```c
struct BackendConfig {
    enum MemoryMode {
        Memory_Normal = 0,
        Memory_High,
        Memory_Low
    };
    
    MemoryMode memory = Memory_Normal;
    
    enum PowerMode {
        Power_Normal = 0,
        Power_High,
        Power_Low
    };
    
    PowerMode power = Power_Normal;
    
    enum PrecisionMode {
        Precision_Normal = 0,
        Precision_High,
        Precision_Low,
        Precision_Low_BF16
    };
    
    PrecisionMode precision = Precision_Normal;
    
    /** user defined context */
    void* sharedContext = nullptr;
};
```

`memory`、`power`、`precision`分别为内存、功耗和精度偏好。支持这些选项的后端会在执行时做出相应调整；若不支持，则忽略选项。

示例：
后端 **OpenCL**
**precision 为 Low 时，使用 fp16 存储与计算**，计算结果与CPU计算结果有少量误差，实时性最好；precision 为 Normal 时，使用 fp16存储，计算时将fp16转为fp32计算，计算结果与CPU计算结果相近，实时性也较好；precision 为 High 时，使用 fp32 存储与计算，实时性下降，但与CPU计算结果保持一致。

后端 CPU
**precision 为 Low 时，根据设备情况开启 FP16 计算**
**precision 为 Low_BF16 时，根据设备情况开启 BF16 计算**

`sharedContext`用于自定义后端，用户可以根据自身需要赋值。

## 关于fallback逻辑

通过调度配置选项，用户可关闭默认的fallback逻辑，或修改fallback后端。

```c++
// dInfer API
#include "dInfer/dInfer_api.h"

// test helper
#include "file_path.h"
#include "test_utils.h"
#include "test/src/post_process/parsing.h"

// third party
#include "gtest/gtest.h"
#include "opencv2/imgcodecs.hpp"
#include "opencv2/dnn/dnn.hpp"

// std
#include <string>
using namespace std;

/**
 * @brief 演示MNN Runtime的fallback逻辑
 * 
 */
#include "dInfer/dInfer_api_mnn.h"
TEST(mnn_runtime, fallback_demo) {
    string test_result_folder = GetCurrentTestDir();

    dInferModelInfo info;
    MNN::ScheduleConfig schedule_config;
    dInferInterface *infer = nullptr;
    
    // 1. 不使用高阶选项时，fallback逻辑会自动生效，device不可用时默认fallback到CPU
    info.device = dInferDevice::DINFER_GPU; // NOTE: DINFER_GPU -> MNN_FORWARD_OPENCL
    info.runtime = dInferEngine::DINFER_MNN;
    info.model_path = std::string(MODELZOO_PATH) + "water/water.mnn";
    info.model_encrypt = false;
    infer = dInferInterfaceCreate(&info);
    ASSERT_NE(infer, nullptr);  // NOTE: 无论设备是否支持GPU都不会返回错误，用户不感知fallback
    ASSERT_EQ(dInferInterfaceDestroy(infer), DINFER_OK); // NOTE: 初始化永远会成功
    infer = nullptr;

    // 2. 使用高阶选项指定forward type，type和backupType一致时则关闭fallback
    schedule_config.type = MNN_FORWARD_VULKAN;  // NOTE: 这里假定MNN未启用VULKAN编译
    schedule_config.backupType = MNN_FORWARD_VULKAN;
    info.optional_attrs["schedule_config"] = reinterpret_cast<void *>(&schedule_config);
    infer = dInferInterfaceCreate(&info);
    ASSERT_EQ(infer, nullptr);  // NOTE: 用户可通过返回值感知到初始化失败

    // 3. 使用高阶选项指定forward type, 可以指定backupType为非CPU的其他类型
    schedule_config.backupType = MNN_FORWARD_OPENCL;
    info.optional_attrs["schedule_config"] = reinterpret_cast<void *>(&schedule_config);
    infer = dInferInterfaceCreate(&info);
    ASSERT_NE(infer, nullptr);  // NOTE: 可以正常fallback非CPU后端
    ASSERT_EQ(dInferInterfaceDestroy(infer), DINFER_OK);
    infer = nullptr;
}
```

## 已弃用选项

以下选项仍然保持兼容，但不建议使用，后续版本可能移除。建议使用**调度配置选项**。

MNN自定义属性配置通过optional_attrs进行配置，optional_attrs的配置有如下：

- `forward_type`:（可选）选择推理后端，默认为CPU，当选择的推理后端在当前设备不支持，则回退到CPU后端.0: CPU； 1: Metal； 2: CUDA；3: OpenCL； 6: OpenGL； 7: Vulkan; 9: TensorRT; */4:AUTO; 5: NNAPI or CoreML; 8: HIAI;
- `number_thread`: （可选）默认值为4. forward type = CPU时，表示线程个数， 为GPU时，表示GPU Tuning模式。TMNNGpuMode, 具体值参考MNNForwardType.h*/
- `precision`:（可选）推理精度， 0: Normal； 1: High； 2: Low； 3: Low_BF16, 默认为2
- `power`:（可选）电量模式， 0: Normal； 1: High； 2: Low；  默认为0
- `memory`:（可选）访存模式， 0: Normal； 1: High； 2: Low；  默认为0

   ```c++
   std::map<std::string, void *> optional_attrs = {
      std::make_pair<std::string, void *>("forward_type", reinterpret_cast<void *>(2))
   };
   ```
